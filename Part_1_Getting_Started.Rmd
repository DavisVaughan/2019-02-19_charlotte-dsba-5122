---
title: "Applied Machine Learning -  Getting Started"
author: Max Kuhn (RStudio)
output:
  xaringan::moon_reader:
    css: ["mtheme_max.css", "fonts_mtheme_max.css"]    
    self_contained: false
    lib_dir: libs
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
---

# Course Overview

> The session will step through the process of building, visualizing, testing and comparing models that are focused on prediction. The goal of the course is to provide a thorough workflow in R that can be used with many different regression or classification techniques. Case studies are used to illustrate functionality.

> _Basic familiarity with R is required._

The *goal* is for you to be able to easily build predictive/machine learning models in R using a variety of packages and model types. 

* "Models that are focused on prediction"... what does that mean?

* "Machine Learning"... so this is deep learning with massive data sets, right? 

The course is broken up into sections for _regression_ (predicting a numeric outcome) and _classification_ (predicting a category). 


---

# Why R for Modeling?

.pull-left[
* _R has cutting edge models_. 
  
  Machine learning developers in some domains use R as their primary computing environment and their work often results in R packages.


* _It is easy to port or link to other applications_. 

  R doesn't try to be everything to everyone. If you prefer models implemented in C, C++, `tensorflow`, `keras`, `python`, `stan`, or `Weka`, you can access these applications without leaving R. 
]

.pull-right[
* _R and R packages are built by people who **do** data analysis_. 

* _The S language is very mature_. 

* The machine learning environment in R is extremely rich. 
]

---

# Downsides to Modeling in R


.pull-left[
* R is a data analysis language and is not C or Java. If a high performance deployment is required, R can be treated like a prototyping language.  

* R is mostly memory-bound. There are plenty of exceptions to this though. 
]

.pull-right[
The main issue is one of _consistency of interface_. For example: 
* There are two methods for specifying what terms are in a model<sup>1</sup>. Not all models have both. 
* 99% of model functions automatically generate dummy variables. 
* Sparse matrices can be used (unless they can't).
]

.footnote[[1] There are now three but the last one is brand new and will be discussed later.]


---

# Syntax for Computing Predicted Class Probabilities

|Function     |Package      |Code                                       |
|:------------|:------------|:------------------------------------------|
|`lda`        |`MASS`       |`predict(obj)`                             |
|`glm`        |`stats`      |`predict(obj, type = "response")`          |
|`gbm`        |`gbm`        |`predict(obj, type = "response", n.trees)` |
|`mda`        |`mda`        |`predict(obj, type = "posterior")`         |
|`rpart`      |`rpart`      |`predict(obj, type = "prob")`              |
|`Weka`       |`RWeka`      |`predict(obj, type = "probability")`       |
|`logitboost` |`LogitBoost` |`predict(obj, type = "raw", nIter)`        |

We'll see a solution for this later in the class. 


---

# Different Philosophies Used Here

There are two main philosophies to data analysis code that will be discussed in this workshop: 

.pull-left[

The more _traditional approach_ uses high-level syntax and is perhaps the most untidy code that you will encounter. 

[`caret`](https://topepo.github.io/caret) is the primary package for untidy predictive modeling: 
1. More traditional R coding style.
1. High-level "I'll do that for you" syntax.
1. More comprehensive (for now) and less modular.
1. Contains many optimizations and is easily parallelized.
]
.pull-right[
The _tidy modeling_ approach espouses the tenets of the [tidyverse](http://www.tidyverse.org/): 
1. Reuse existing data structures.
1. Compose simple functions with the pipe.
1. Embrace functional programming.
1. Design for humans.

This approach is exemplified by the new set of tidyverse package... 
]


---

# `tidymodels` Collection of Packages  <img src="images/tidymodels_hex.png" class="title-hex">


```{r tm}
library(tidymodels)
```

Plus [`tidypredict`](http://tidypredict.netlify.com/), [`tidyposterior`](https://tidymodels.github.io/tidyposterior/), [`tidytext`](https://github.com/juliasilge/tidytext), and more in development.

```{r ggplot, include = FALSE}
thm <- theme_bw() + 
  theme(
    panel.background = element_rect(fill = "transparent", colour = NA), 
    plot.background = element_rect(fill = "transparent", colour = NA),
    legend.position = "top",
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)
```

---

# Example Data Set - House Prices

For regression problems, we will use the Ames IA housing data. There are 2,930 properties in the data. 

The Sale Price was recorded along with 81 predictors, including:

* Location (e.g. neighborhood) and lot information.
* House components (garage, fireplace, pool, porch, etc.).
* General assessments such as overall quality and condition.
* Number of bedrooms, baths, and so on. 

More details can be found in [De Cock (2011, Journal of Statistics Education)](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf).

The raw data are at [`http://bit.ly/2whgsQM`](http://bit.ly/2whgsQM) but we will use a processed version found in the [`AmesHousing`](https://github.com/topepo/AmesHousing) package. 


---

# Example Data Set - House Prices

```{r ames-map, echo = FALSE, message = FALSE, fig.align='center', dev = "svg"}
library(AmesHousing)
library(leaflet)
library(htmltools)
library(Cairo)
ames <- make_ames()

col_key <- c(
  'NAmes',     '#0000FF',
  'CollgCr',   '#FF0000',
  'OldTown',   '#FFFFFF',
  'Edwards',   '#FF00B6',
  'Somerst',   '#FF3030',
  'NridgHt',   '#009FFF',
  'Gilbert',   '#DD00FF',
  'Sawyer',    '#9A4D42',
  'NWAmes',    '#00FFBE',
  'SawyerW',   '#1F9698',
  'Mitchel',   '#FFACFD',
  'BrkSide',   '#720055',
  'Crawfor',   '#F1085C',
  'IDOTRR',    '#FE8F42',
  'Timber',    '#004CFF',
  'NoRidge',   '#ffff00',
  'StoneBr',   '#B1CC71',
  'SWISU',     '#02AD24',
  'ClearCr',   '#FFD300',
  'MeadowV',   '#886C00',
  'BrDale',    '#FFB79F',
  'Blmngtn',   '#858567',
  'Veenker',   '#A10300',
  'NPkVill',   '#00479E',
  'Blueste',   '#DC5E93',
  'Greens',    '#93D4FF',
  'GreenHills', '#e5f2e5', 
  'Landmrk',   '#C8FF00'
) 
col_key <- as.data.frame(matrix(col_key, byrow = TRUE, ncol = 2))
names(col_key) <- c("Neighborhood", "color")
col_key <- col_key %>%
    mutate(
      Neighborhood =
        dplyr::recode(
          Neighborhood,
          "Blmngtn" = "Bloomington_Heights",
          "Bluestem" = "Bluestem",
          "BrDale" = "Briardale",
          "BrkSide" = "Brookside",
          "ClearCr" = "Clear_Creek",
          "CollgCr" = "College_Creek",
          "Crawfor" = "Crawford",
          "Edwards" = "Edwards",
          "Gilbert" = "Gilbert",
          "Greens" = "Greens",
          "GreenHills" = "Green_Hills",
          "IDOTRR" = "Iowa_DOT_and_Rail_Road",
          "Landmrk" = "Landmark",
          "MeadowV" = "Meadow_Village",
          "Mitchel" = "Mitchell",
          "NAmes" = "North_Ames",
          "NoRidge" = "Northridge",
          "NPkVill" = "Northpark_Villa",
          "NridgHt" = "Northridge_Heights",
          "NWAmes" = "Northwest_Ames",
          "OldTown" = "Old_Town",
          "SWISU" = "South_and_West_of_Iowa_State_University",
          "Sawyer" = "Sawyer",
          "SawyerW" = "Sawyer_West",
          "Somerst" = "Somerset",
          "StoneBr" = "Stone_Brook",
          "Timber" = "Timberland",
          "Veenker" = "Veenker"
        ))

lon_rnd <- range(ames$Longitude)
lat_rnd <- range(ames$Latitude)

ia_map <- leaflet(width = "100%") %>%
  addProviderTiles(providers$Stamen.Toner)

for(i in 1:nrow(col_key)) {
  ia_map <- ia_map %>%
    addCircles(
      data = subset(ames, Neighborhood == col_key$Neighborhood[i]),
      lng = ~Longitude, lat = ~Latitude,
      color = col_key$color[i],
      fill = TRUE,
      fillColor = col_key$color[i],
      radius = 6,
      popup = htmlEscape(col_key$Neighborhood[i]),
      opacity = .25)
}
ia_map
```

---

```{r, download_cars, include=FALSE, message=FALSE, warning=FALSE}
url <- "https://github.com/topepo/cars/raw/master/2018_10_24_city/car_data_splits.RData"
temp_save <- tempfile()
download.file(url, destfile = temp_save)
load(temp_save)
```


# Example Data Set - Fuel Economy

The data that are used here are an extended version of the ubiquitous `mtcars` data set. 

[`fueleconomy.gov`](https://www.fueleconomy.gov/feg/download.shtml) was used to obtain fuel efficiency data on cars from 2015-2019. 

Over this time range, duplicate ratings were eliminated; these occur when the same car is sold for several years in a row. As a result, there are `r nrow(car_train)+nrow(car_test)` cars that are listed in the data. The predictors include the automaker and additional information about the cars (e.g. cylinders, etc). 

In our analysis, the data from 2015-2018 are used for training to see if we can predict the `r nrow(car_test)` cars that were new in 2019. We will restrict ourselves to cars that use some type of gas. 

These data are supplied in the GitHub repo.


---

# Example Data Set - Predicting Profession

OkCupid is an online data site that serves international users. [Kim and Escobedo-Land (2015, Journal of Statistics Education)](http://ww2.amstat.org/publications/jse/v23n2/kim.pdf) describe a data set where over 50,000 profiles from the San Fransisco area were made available by the company. 

The data contains several types of fields:

* A number of open text essays related to interests and personal descriptions 
* Single choice type fields, such as profession, diet, gender, body type, etc.
* Multiple choice data, including languages spoken, etc.
* ***No*** usernames or pictures were included. 

We will try to predict whether someone has a profession in the STEM fields (science, technology, engineering, and math) using a random sample of the overall dataset. 


---

# Tidyverse Syntax <img src="images/dplyr.png" class="title-hex">

Many tidyverse functions have syntax unlike base R code. For example:

* Vectors of variable names are eschewed in favor of _functional programming_. For example: 

```{r func-ex, eval = FALSE}
contains("Sepal")

# instead of 

c("Sepal.Width", "Sepal.Length")
```

* The _pipe_ operator is preferred. For example

```{r pipe-ex, eval = FALSE}
merged <- inner_join(a, b)

# is equal to

merged <- a %>%
  inner_join(b) 
```

* Functions are more _modular_ than their traditional analogs (`dplyr`'s `filter()` and `select()` vs `base::subset()`)

---

# Some Example Data Manipulation Code <img src="images/dplyr.png" class="title-hex"><img src="images/readr.png" class="title-hex">

```{r tidy-example, message = FALSE}
library(tidyverse)

ames_prices <- "http://bit.ly/2whgsQM" %>%
  read_delim(delim = "\t") %>%
  rename_at(vars(contains(' ')), funs(gsub(' ', '_', .))) %>%
  rename(Sale_Price = SalePrice) %>%
  filter(!is.na(Electrical)) %>%
  select(-Order, -PID, -Garage_Yr_Blt)

ames_prices %>%
  group_by(Alley) %>%
  summarize(
    mean_price = mean(Sale_Price / 1000),
    n = sum(!is.na(Sale_Price))
  )
```


---

# Example `ggplot2` Code <img src="images/ggplot2.png" class="title-hex">

.pull-left[

```{r ggplot-example, eval = FALSE}
library(ggplot2)

ames_prices %>%
  ggplot(
    aes(
      x = Garage_Type, 
      y = Sale_Price
    )
  ) + 
  geom_violin() + 
  coord_trans(y = "log10") + 
  xlab("Garage Type") + 
  ylab("Sale Price") 
```
]

.pull-left[
```{r ggplot - example - disp, echo = FALSE, fig.width = 6, fig.height = 4.25,  out.width = '100%', fig.align = 'center', dev = 'svg', dev.args = list(bg = "transparent")}
library(ggplot2)

ggplot(ames_prices, 
       aes(x = Garage_Type,
           y = Sale_Price)) + 
  geom_violin() + 
  coord_trans(y = "log10") + 
  xlab("Garage Type") + 
  ylab("Sale Price") 
```
]


---

# Examples of `purrr::map*` <img src="images/dplyr.png" class="title-hex"><img src="images/purrr.png" class="title-hex">

purrr contains functions that _iterate over lists_ without the explicit use of loops. They are similar to the family of apply functions in base R, but are type stable.



.pull-left[

```{r purrr-example-lhs}
library(purrr)

mini_ames <- ames_prices %>%
  select(Alley, Sale_Price, Yr_Sold) %>%
  filter(!is.na(Alley))

head(mini_ames, n = 5)
```

]

.pull-right[
```{r purrr-split-map}
by_alley <- split(mini_ames, mini_ames$Alley)
map(by_alley, head, n = 2)
```

]

---

# Examples of `purrr::map*` <img src="images/dplyr.png" class="title-hex"><img src="images/purrr.png" class="title-hex">

.pull-left[
```{r purrr-map-nrow}
map(by_alley, nrow)
```

`map()` always returns a list. Use suffixed versions for simplification of the result.

```{r purrr-map-int}
map_int(by_alley, nrow)
```

]

.pull-right[

Complex operations can be specified using a _formula notation_. Access the current
thing you are iterating over with `.x`.

```{r purrr-map-summarise}
map(
  by_alley, 
  ~summarise(.x, max_price = max(Sale_Price))
)
```

]

---

# `purrr` and list-columns <img src="images/dplyr.png" class="title-hex"><img src="images/purrr.png" class="title-hex"><img src="images/tidyr.png" class="title-hex">

Rather than using `split()`, we can `tidyr::nest()` by `Alley` to get a data frame with
a _list-column_. We often use these when working with _multiple models_.

.pull-left[

```{r tidyr-nest}
ames_lst_col <- nest(mini_ames, -Alley)
ames_lst_col
```

]

.pull-right[

```{r list-col-mutate}
ames_lst_col %>%
  mutate(
    n_row = map_int(data, nrow),
    max   = map_dbl(data, ~max(.x$Sale_Price))
  )
```

]

---

# List-columns and `unnest()` <img src="images/purrr.png" class="title-hex"><img src="images/tidyr.png" class="title-hex">

`unnest()` repeats regular columns once for each row of the unnested list-column. `"Pave"` is repeated 78 times, and `"Grvl"` is repeated 120 times.

.pull-left[

```{r list-col-print}
ames_lst_col
```

You can unnest multiple list-columns at once if they have the same number of rows. We will use this when unnesting predictions for each resample.

]

.pull-right[

```{r unnest-example}
unnest(ames_lst_col, data)
```

]



---

# Quasiquotation <img src="images/rlang.png" class="title-hex">

The tidyverse benefits from the occasional use of [metaprogramming techniques](https://adv-r.hadley.nz/introduction-16.html) techniques. 

The main thing that we will use is [_quasiquotation_](https://adv-r.hadley.nz/quasiquotation.html) in which we can splice arguments into an expression. 

For example, suppose we have a character vector of variable names that we'd like to keep in a `select` statement. We often just type things out:

```{r select-iris}
mtcars %>% select(mpg, wt, hp) %>% slice(1:2)
```

But what if that list is very long or is generated programmatically? How do we emulate a comma separated list of argument values? 

---
# Quasiquotation <img src="images/rlang.png" class="title-hex">

Quasiquotation has an operator, `!!!` (read as "triple-bang"), that can _splice in_ multiple values into an argument. For example:

```{r select-car-bang-bang-bang}
cols <- c("mpg", "wt", "hp")
mtcars %>% select(!!!cols) %>% names()
```

There is also a `!!` operator that is used when the argument can only have one value (i.e. _not_ `...`) 

```{r select-car-bang-bang}
value <- 5
mtcars %>% select(!!!cols) %>% mutate(x = !!value) %>% slice(1:2)
```

**Remember**: `!!` for `foo(x)` and `!!!` for `bar(...)`. 


---

# Quick Data Investigation

To get warmed up, let's load the real Ames data and do some basic investigations into the variables, such as exploratory visualizations or summary statistics. The idea is to get a feel for the data. 

Let's take 10 minutes to work on your own or with someone next to you. Collaboration is highly encouraged!

To get the data:

```{r load-ames}
library(AmesHousing)
ames <- make_ames()
```


---

# Where We Go From Here

**Part 2** Basic Principles

 * Data Splitting, Resampling, Tuning (`rsample`)
 
 * Models in R (`parsnip`)


**Part 3** Feature Engineering and Preprocessing

 * Data treatments (`recipes`)
 
 
**Part 4** Regression Modeling

 * Measuring Performance, penalized regression, multivariate adaptive regression splines (MARS), ensembles  (`yardstick`, `recipes`, `caret`, `earth`, `glmnet`, `tidyposterior`, `doParallel`)
 
 
**Part 5** Classification Modeling

 * Measuring Performance, trees, ensembles, naive Bayes (`yardstick`, `recipes`, `caret`, `rpart`, `klaR`, `tidyposterior`)

---

# Resources

* [`http://www.tidyverse.org/`](http://www.tidyverse.org/)
* [R for Data Science](http://r4ds.had.co.nz/)
* Jenny's  [`purrr` tutorial](https://jennybc.github.io/purrr-tutorial/) or [Happy R Users Purrr](https://www.rstudio.com/resources/videos/happy-r-users-purrr-tutorial/)
* Programming with `dplyr` [vignette](https://cran.r-project.org/web/packages/dplyr/vignettes/programming.html)
* Selva Prabhakaran's [`ggplot2` tutorial](http://r-statistics.co/Complete-Ggplot2-Tutorial-Part1-With-R-Code.html)
* `caret` package [documentation](https://topepo.github.io/caret/)
* [CRAN Machine Learning Task View](https://cran.r-project.org/web/views/MachineLearning.html)

About these slides.... they were created with Yihui's [`xaringan`](https://github.com/yihui/xaringan) and the stylings are a slightly modified version of Patrick Schratz's  [Metropolis theme](https://github.com/pat-s/xaringan-metropolis).
