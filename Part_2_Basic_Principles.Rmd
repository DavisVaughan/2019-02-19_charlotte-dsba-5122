---
title: "Applied Machine Learning -  Basic Principles"
author: Max Kuhn (RStudio)
output:
  xaringan::moon_reader:
    css: ["mtheme_max.css", "fonts_mtheme_max.css"]  
    self_contained: false
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
library(caret)
library(ggplot2)
library(leaflet)
library(AppliedPredictiveModeling)
library(ggthemes)
library(DiagrammeR)

thm <- theme_bw() + 
  theme(
    panel.background = element_rect(fill = "transparent", colour = NA), 
    plot.background = element_rect(fill = "transparent", colour = NA),
    legend.position = "top",
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)
```

# Load Packages  <img src="images/tidymodels_hex.png" class="title-hex">

.code70[
```{r tm-load, warning = FALSE}
library(tidymodels)
```
]

---
# Introduction

In this section, we will introduce concepts that are useful for any type of machine learning model:

* _modeling_ versus the model

* data splitting

* resampling

* tuning parameters and overfitting

* model tuning


Many of these topics will be put into action in later sections.


---

# The Modeling _Process_

Common steps during model building are:

* estimating model parameters (i.e. training models)

* determining the values of _tuning parameters_ that cannot be directly calculated from the data

* model selection (within a model type) and model comparison (between types)

* calculating the performance of the final model that will generalize to new data

Many books and courses portray predictive modeling as a short sprint. A better analogy would be a marathon or campaign (depending on how hard the problem is). 


---

# What the Modeling Process Usually Looks Like

```{r mod-process, echo = FALSE, out.width = '95%', fig.width=8, fig.height=2.5, fig.align='center', dev = 'svg', dev.args = list(bg = "transparent"), warning=FALSE}
widths <- c(8, 4, 10, 2, 6, 6, 
            rep(1, 19), 2,
            rep(1, 19), 2,
            rep(1, 19), 2,
            rep(1, 19), 2,
            4, 8, 15,
            rep(1, 29), 6,
            rep(1, 29), 4,
            1)
srt <- c(1, cumsum(widths))
stp <- srt[-1]
srt <- srt[-length(srt)]

diag_cols <- c(EDA = "#377EB8", "Quantitative Analysis" = "#A6CEE3", 
               "Feature Engineering" = "#4DAF4A", "Model Fit" = "#E41A1C", 
               "Model Tuning" = "grey")

bar_loc <- data.frame(srt = srt,
                  stp = stp,
                  g = c("EDA", "Quantitative Analysis", "EDA", "Quantitative Analysis", "EDA", "Feature Engineering", 
                        rep(c("Model Fit", "Model Tuning"), 40),
                        "Quantitative Analysis", "EDA", "Feature Engineering",
                        rep(c("Model Fit", "Model Tuning"), 14), "Model Fit", "Feature Engineering",
                        rep(c("Model Fit", "Model Tuning"), 14), "Model Fit", "Quantitative Analysis",
                        "Model Fit"))
bar_loc$ytop <- 1.9
bar_loc$ybot <- 1
bar_loc$g <- factor(as.character(bar_loc$g), 
                levels = c("EDA", "Quantitative Analysis", "Feature Engineering",
                           "Model Fit", "Model Tuning"))
text_loc <- data.frame(x = c(1, 8, 30, 36, 120, 124, 132, 147, 211, 215)+1,
                       y = 2.1)
text_loc$label <- letters[1:nrow(text_loc)]

mod_loc <- data.frame(x = c(45, 66, 87, 107, 162, 195)+1,
                      y = .75, 
                      label = c("Model\n#1", "Model\n#2", "Model\n#3", "Model\n#4",
                                "Model\n#2", "Model\n#4"))

ggplot(bar_loc) + 
  geom_rect(aes(fill = g, xmin = srt, xmax = stp,
                ymin = ybot, ymax = ytop), alpha = .7)  + 
  theme(
    legend.position = "bottom",
    legend.background = element_blank(),
    axis.line = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks = element_blank(),
    axis.title.x = element_text(hjust = .05),
    axis.title.y = element_blank(),
    panel.background = element_blank(),
    panel.border = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    plot.background = element_blank()
    ) +
  scale_fill_manual(values = diag_cols, name = "") +
  geom_text(data = text_loc, aes(x = x, y = y, label = label)) + 
  geom_text(data = mod_loc, aes(x = x, y = y, label = label), size = 3) +   
  xlab("Time") + 
  ylim(c(.5, 2.25))
```

---
layout: false
class: inverse, middle, center

#  Data Usage

---

# Data Splitting and Spending

How do we "spend" the data to find an optimal model? 

We _typically_ split data into training and test data sets:

*  ***Training Set***: these data are used to estimate model parameters and to pick the values of the complexity parameter(s) for the model.

*  ***Test Set***: these data can be used to get an independent assessment of model efficacy. They should not be used during model training. 


---

# Data Splitting and Spending 

The more data we spend, the better estimates we'll get (provided the data is accurate).  

Given a fixed amount of data:

* too much spent in training won't allow us to get a good assessment of predictive performance.  We may find a model that fits the training data very well, but is not generalizable (overfitting)

* too much spent in testing won't allow us to get a good assessment of model parameters

Statistically, the best course of action would be to use all the data for model building and use statistical methods to get good estimates of error.

From a non-statistical perspective, many consumers of complex models emphasize the need for an untouched set of samples to evaluate performance.


---

# Large Data Sets

When a large amount of data are available, it might seem like a good idea to put a large amount into the training set. _Personally_, I think that this causes more trouble than it is worth due to diminishing returns on performance and the added cost and complexity of the required infrastructure. 

Alternatively, it is probably a better idea to reserve good percentages of the data for specific parts of the modeling process. For example: 

* Save a large chunk of data to perform feature selection prior to model building
* Retain data to calibrate class probabilities or determine a cutoff via an ROC curve. 

Also, there may be little need for iterative resampling of the data. A single holdout (aka validation set) may be sufficient in some cases if the data are large enough and the data sampling mechanism is solid.  


---

# Mechanics of Data Splitting

There are a few different ways to do the split: simple random sampling, _stratified sampling based on the outcome_, by date, or methods that focus on the distribution of the predictors.

For stratification:

* **classification**: this would mean sampling within the classes to preserve the distribution of the outcome in the training and test sets

* **regression**: determine the quartiles of the data set and sample within those artificial groups


---

# Ames Housing Data <img src="images/rsample.png" class="title-hex">

Let's load the example data set and split it. We'll put 75% into training and 25% into testing. 

```{r ames-split, message = FALSE}
library(AmesHousing)
ames <- 
  make_ames() %>% 
  # Remove quality-related predictors
  dplyr::select(-matches("Qu"))
nrow(ames)

# Make sure that you get the same random numbers
set.seed(4595)
data_split <- initial_split(ames, strata = "Sale_Price")

ames_train <- training(data_split)
ames_test  <- testing(data_split)

nrow(ames_train)/nrow(ames)
```
???

The select statement removes subjective quality scores which, to me, seems
like it should be an outcome and not a predictor. 

---

# Ames Housing Data <img src="images/rsample.png" class="title-hex">

What do these objects look like?

```{r}
# result of initial_split()
# <training / testing / total>
data_split
```

```{r, eval=FALSE}
training(data_split)
```

```{r}
## # A tibble: 2,199 x 81
##    MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape Land_Contour Utilities Lot_Config Land_Slope
##    <fct>       <fct>            <dbl>    <int> <fct>  <fct> <fct>     <fct>        <fct>     <fct>      <fct>     
##  1 One_Story_… Resident…          141    31770 Pave   No_A… Slightly… Lvl          AllPub    Corner     Gtl       
##  2 Two_Story_… Resident…           74    13830 Pave   No_A… Slightly… Lvl          AllPub    Inside     Gtl       
##  3 Two_Story_… Resident…           78     9978 Pave   No_A… Slightly… Lvl          AllPub    Inside     Gtl       
##  4 One_Story_… Resident…           43     5005 Pave   No_A… Slightly… HLS          AllPub    Inside     Gtl       
##  5 One_Story_… Resident…           39     5389 Pave   No_A… Slightly… Lvl          AllPub    Inside     Gtl       
## # … and many more rows and columns
## # …
```


---

# Outcome Distributions <img src="images/ggplot2.png" class="title-hex">

.pull-left[

```{r ames-split-dists, eval = FALSE}
## Do the distributions line up? 
ggplot(ames_train, aes(x = Sale_Price)) + 
  geom_line(stat = "density", 
            trim = TRUE) + 
  geom_line(data = ames_test, 
            stat = "density", 
            trim = TRUE, col = "red")
```

]

.pull-right[

```{r ames-split-dists-dist, echo = FALSE, fig.width=6, fig.height=4.25,  out.width = '100%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}
ggplot(ames_train, aes(x = Sale_Price)) + 
  geom_line(stat = "density", trim = TRUE) + 
  geom_line(data = ames_test, 
            stat = "density", 
            trim = TRUE, col = "red") 
```

]


---
layout: false
class: inverse, middle, center

#  Creating Models in R


---

# Specifying Models in R Using Formulas

To fit a model to the housing data, the model terms must be specified. Historically, there are two main interfaces for doing this. 

The **formula** interface using R [formula rules](https://cran.r-project.org/doc/manuals/r-release/R-intro.html#Formulae-for-statistical-models) to specify a _symbolic_ representation of the terms:

Variables + interactions

```{r formula-1, eval = FALSE}
model_fn(Sale_Price ~ Neighborhood + Year_Sold + Neighborhood:Year_Sold, data = ames_train)
```

Shorthand for all predictors

```{r formula-2, eval = FALSE}
model_fn(Sale_Price ~ ., data = ames_train)
```

Inline functions / transformations

```{r formula-3, eval = FALSE}
model_fn(log10(Sale_Price) ~ ns(Longitude, df = 3) + ns(Latitude, df = 3), data = ames_train)
```

This is very convenient but it has some disadvantages.  

---

# Downsides to Formulas

* You can't nest in-line functions such as `model_fn(y ~ pca(scale(x1), scale(x2), scale(x3)), data = dat)`.

* All the model matrix calculations happen at once and can't be recycled when used in a model function. 

* For very _wide_ data sets, the formula method can be [extremely inefficient](https://rviews.rstudio.com/2017/03/01/the-r-formula-method-the-bad-parts/). 

* There are limited _roles_ that variables can take which has led to several re-implementations of formulas. 

* Specifying multivariate outcomes is clunky and inelegant.

* Not all modeling functions have a formula method (consistency!). 

---

# Specifying Models Without Formulas

Some modeling function have a non-formula (XY) interface. This usually has arguments for the predictors and the outcome(s):

```{r non-formula, eval = FALSE}
# Usually, the variables must all be numeric
pre_vars <- c("Year_Sold", "Longitude", "Latitude")
model_fn(x = ames_train[, pre_vars],
         y = ames_train$Sale_Price)
```

This is inconvenient if you have transformations, factor variables, interactions, or any other operations to apply to the data prior to modeling. 

Overall, it is difficult to predict if a package has one or both of these interfaces. For example, `lm` only has formulas. 

There is a **third interface**, using _recipes_ that will be discussed later that solves some of these issues. 

---

# A Linear Regression Model <img src="images/broom.png" class="title-hex">

Let's start by fitting an ordinary linear regression model to the training set. You can choose the model terms for your model, but I will use a very simple model:

```{r lm-1}
simple_lm <- lm(log10(Sale_Price) ~ Longitude + Latitude, data = ames_train)
```

Before looking at coefficients, we should do some model checking to see if there is anything obviously wrong with the model. 

To get the statistics on the individual data points, we will use the awesome `broom` package:

```{r lm-broom, warning= FALSE, message= FALSE}
simple_lm_values <- augment(simple_lm)
names(simple_lm_values)
``` 


---

# Hands-On: Some Basic Diagnostics 

From these results, let's take 10 minutes and do some visualizations: 

* Plot the observed versus fitted values

* Plot the residuals

* Plot the predicted versus residuals

Are there any _downsides_ to this approach? 

---

# parsnip <img src="images/parsnip.png" class="title-hex">

- A tidy unified _interface_ to models

- `lm()` isn't the only way to perform linear regression
  
  - `glmnet` for regularized regression
  
  - `stan` for Bayesian regresion
  
  - `keras` for regression using tensorflow
  
- But...remember the consistency slide?

  - Each interface has its own minutae to remember
  
  - `parsnip` standardizes all that!
  
---

# parsnip in Action <img src="images/parsnip.png" class="title-hex">

.pull-left[

1) Create specification

2) Set the engine

3) Fit the model

```{r}
spec_lin_reg <- linear_reg()
spec_lin_reg

spec_lm <- set_engine(spec_lin_reg, "lm")
spec_lm
```

]

.pull-right[

```{r}
fit_lm <- fit(
  spec_lm,
  log10(Sale_Price) ~ Longitude + Latitude,
  data = ames_train
)

fit_lm
```

]

---

# Different interfaces <img src="images/parsnip.png" class="title-hex">

`parsnip` is not picky about the interface used to specify terms. Remember, `lm()` only allowed the formula interface!

```{r}
ames_train_log <- ames_train %>%
  mutate(Sale_Price_Log = log10(Sale_Price))

fit_xy(
  spec_lm,
  y = ames_train_log$Sale_Price_Log,
  x = ames_train_log[, c("Latitude", "Longitude")]
)
```


---

# Alternative Engines <img src="images/parsnip.png" class="title-hex">

With `parsnip`, it is easy to switch to a different engine, like Stan, to run the
same model with alternative backends.

.pull-left[

```{r, results = "hide", cache=TRUE}
spec_stan <- 
  spec_lin_reg %>%
  # Engine specific arguments are passed through here
  set_engine("stan", chains = 4, iter = 1000)

# Otherwise, looks exactly the same!
fit_stan <- fit(
  spec_stan,
  log10(Sale_Price) ~ Longitude + Latitude,
  data = ames_train
)
```

]

.pull-right[

```{r}
coef(fit_stan$fit)

coef(fit_lm$fit)
```

]

---
layout: false
class: inverse, middle, center

#  Model Evaluation


---

# Overall Model Statistics 

`parsnip` holds the actual model object in the `fit_lm$fit` slot. If you use the `summary()` method on the underlying `lm` object, the bottom shows some statistics: 

```{r lm-stats, eval = FALSE}
summary(fit_lm$fit)
```

```{r lm-stats-disp, echo = FALSE}
summary_res <- capture.output(summary(fit_lm$fit))
nlines <- length(summary_res)
summary_res <- summary_res[grep("Residual standard error", summary_res):(nlines-1)]
summary_res <- c("<snip>", summary_res)
cat(summary_res, sep = "\n")
```

These statistics are generated from _predicting on the training data used to fit the model_. This is problematic because it can lead to optimistic results, especially for flexible models (overfitting). 

--

### Idea!

The tests set is used for assessing performance. **Should we predict the test set** and use those results to estimate these statistics? 

---

<img src="images/nope.png" align = "middle" height = "400px" float = "center">

(Matthew Inman/Exploding Kittens)

---

# Assessing Models

_Save the test set_ until the very end when you have one or two models that are your favorite. We need to use the training set...but how?

--

.pull-left[

### Maybe... 

1) For model A, fit on training set, predict on training set

2) For model B, fit on training set, predict on training set

3) Compare performance

]

--

.pull-right[

For some models, it is possible to get very "good" performance by predicting the training set (it was so flexible you overfit it). That's an issue since we will need to make "honest" comparisons between models before we finalize them and run our final choices on the test set.

### If only...

If only we had a method for getting honest performance estimates from the _training set_...

]

---

# Resampling Methods

.pull-left[
These are additional data splitting schemes that are applied to the _training_ set. 

They attempt to simulate slightly different versions of the training set. These versions of the original are split into two model subsets:

* The _analysis set_ is used to fit the model (analogous to the training set). 
* Performance is determined using the _assessment set_. 

This process is repeated many times. 

There are different flavors or resampling but we will focus on two methods. 

]

.pull-right[

```{r split-graph, out.width = '100%', echo = FALSE, message=FALSE, warning=FALSE}
grViz("
digraph resampling_diag {
      
graph [layout = dot, bgcolor = transparent]

node [fontname = Helvetica]

all [shape = circle,
     label = 'All\nData']

te [shape = circle,
    style = filled,
    label = 'Testing',
    fillcolor = '#eeeeb4']

tr [shape = circle,
    style = filled,
    label = 'Training',
    fillcolor = '#c8d8c2']

r1 [shape = rectangle,
    label = 'Resample 1']

an1 [shape = oval,
     style = filled,
     label = 'Analysis',
     fillcolor = honeydew]
as1 [shape = oval,
     style = filled,
     label = 'Assessment',
     fillcolor = ivory]

r2 [shape = rectangle,
    label = 'Resample 2']
      
an2 [shape = oval,
     style = filled,
     label = 'Analysis',
     fillcolor = honeydew]

as2 [shape = oval,
     style = filled,
     label = 'Assessment',
     fillcolor = ivory]

r3 [shape = rectangle,
    label = 'Resample 3']
      
an3 [shape = oval,
     style = filled,
     label = 'Analysis',
     fillcolor = honeydew]
as3 [shape = oval,
     style = filled,
     label = 'Assessment',
     fillcolor = ivory]

all -> {tr te }
tr -> {r1 r2 r3 }
r1 -> {as1 an1 }
r2 -> {an2 as2 }
r3 -> {an3 as3 }
}
")
```

]

---

# V-Fold Cross-Validation

.pull-left[

Here, we randomly split the training data into _V_ distinct blocks of roughly equal size.

* We leave out the first block of analysis data and fit a model.

* This model is used to predict the held-out block of assessment data.

* We continue this process until we've predicted all _V_ assessment blocks

The final performance is based on the hold-out predictions by _averaging_ the statistics from the _V_ blocks. 

]

.pull-right[

```{r rs-diagram, echo = FALSE, out.width = '95%', fig.width=6, fig.height=2.5, fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}
grd <- 
  expand.grid(Resample = 1:5, Set = 1:5) %>%
  mutate(
    Purpose = ifelse(Resample == Set, "Performance", "Modeling"),
    Resample = factor(paste("Resample", Resample), levels = paste("Resample", 5:1))
  ) 

ggplot(grd, aes(x = Set, y = Resample, fill = Purpose)) + 
  geom_tile(width = 0.90, height = 0.85) + 
  xlab("< ------------------------ Random Data Groupings ------------------------>") + 
  ylab("") + 
  scale_fill_manual(values=c("#F46D43", "#dedede"))+ 
  thm + 
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank()
  )
```

<br>

_V_ is usually taken to be 5 or 10 and leave one out cross-validation has each sample as a block. 


]

---

#  10-Fold Cross-Validation with _n_ = 50

```{r cv-plot, echo = FALSE, message = FALSE, fig.width=6, fig.height=4.25,  out.width = '60%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}

set.seed(4121)
cv10 <- vfold_cv(ames[1:50, ])
cv10 <- tidy(cv10) %>%
  mutate(Row = factor(Row, levels = paste(1:50))) 

ggplot(cv10, aes(x = Fold, y = Row, fill = Data)) + 
  geom_tile() + scale_fill_brewer(palette = "Paired") + 
  xlab("") + ylab("Training Set Sample") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        axis.text.y = element_text(size = rel(.5)), 
        legend.position = "top",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
```


---

# Bootstrapping

A bootstrap sample is the _same size_ as the training set but each data point is selected _with replacement_. 

* _Analysis set_

  Will contain more than one replicate of a training set instance.
  
* _Assessment set_

  Contains all samples that were never included in the corresponding bootstrap set.
  Often called the "out-of-bag" sample and can vary in size!

On average, `r (1-exp(-1))*100`% of the training set is contained _at least once_ in the bootstrap sample. 

---

#  Bootstrapping with _n_ = 50

```{r boot-plot, echo = FALSE, warning=FALSE, message = FALSE, fig.width=6, fig.height=4.25,  out.width = '60%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}
set.seed(4121)
bt_rows <- bootstraps(ames[1:50, ], times = 10)

bt_rows <- data.frame(Row = unlist(lapply(bt_rows$splits, function(x) sort(x$in_id))),
                      Resample = rep(recipes:::names0(10, "Resample"), each = 50))

bt_rows <- bt_rows %>%
  group_by(Resample, Row) %>%
  summarize(Replicates = length(Row)) %>%
  mutate(Row = factor(Row, levels = paste(1:50)),
         Replicates = factor(Replicates))

ggplot(bt_rows, aes(x = Resample, y = Row, fill = Replicates)) + 
  geom_tile() + scale_fill_brewer() + 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) + 
  xlab("") + ylab("Training Set Sample") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        axis.text.y = element_text(size = rel(.5)), 
        legend.position = "top",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
```


---

# Comparing Resampling Methods

If you think of resampling in the same manner as statistical estimators (e.g. maximum likelihood), this becomes a trade-off between bias and variance:

 * Variance is (mostly) driven by the number of resamples (e.g. 5-fold CV has larger variance than 10-fold). 
 * Bias is (mostly) related to how much data is held back. The bootstrap has large bias compared to 10-fold CV. 

There are lengthy blog posts about this subject [here](http://bit.ly/1yE0Ss5) and [here](http://bit.ly/1zfoFj2). 

I tend to favor 5 repeats of 10-fold cross-validation unless the size of the assessment data is "large enough". 

For example, 10% of the Ames training set is `r floor(nrow(ames_train)*.1)` properties and this is probably good enough to estimate the RMSE and _R_<sup>2</sup>.  


---

# Cross-Validating Using `rsample` <img src="images/rsample.png" class="title-hex">

.pull-left[

```{r cv-ames}
set.seed(2453)
cv_splits <- vfold_cv(
  data = ames_train, 
  v = 10, 
  strata = "Sale_Price"
)
cv_splits %>% slice(1:6)
```

]

.pull-right[

Each individual split object is similar to the 
`initial_split()` example.

```{r cv-ames-splits}
cv_splits$splits[[1]]

cv_splits$splits[[1]] %>% analysis() %>% dim()
cv_splits$splits[[1]] %>% assessment() %>% dim()
```

]

???

Note that `<split [2K/222]>` rounds to the thousandth and is the same as `<1977/222/2199>`


---

# Resampling the Linear Model <img src="images/rsample.png" class="title-hex"><img src="images/purrr.png" class="title-hex"><img src="images/parsnip.png" class="title-hex"><img src="images/dplyr.png" class="title-hex">

Working with resample tibbles generally involves two things:

1) Small functions that perform an action on a single split.

2) The `purrr` package for `map()`ping over splits.

.pull-left[

```{r, eval=FALSE}
geo_form <- log10(Sale_Price) ~ Latitude + Longitude

# Fit on a single analysis resample
fit_model <- function(split, spec) {
  fit(
    object = spec, 
    formula = geo_form,
    data = analysis(split) # <- pull out training set
  )
}

# For each resample, call fit_model()
cv_splits <- cv_splits %>% 
  mutate(models_lm = map(splits, fit_model, spec_lm))

cv_splits
```

]

.pull-right[

```{r, echo=FALSE}
geo_form <- log10(Sale_Price) ~ Latitude + Longitude

# Fit on a single analysis resample
fit_model <- function(split, spec) {
  fit(
    object = spec, 
    formula = geo_form,
    data = analysis(split) # <- pull out training set
  )
}

# For each resample, call fit_model()
cv_splits <- cv_splits %>% 
  mutate(models_lm = map(splits, fit_model, spec_lm))

cv_splits
```

]

???

Note that `<fit[+]>` means not model fitting failures.


---

# Resampling the Linear Model <img src="images/purrr.png" class="title-hex"><img src="images/parsnip.png" class="title-hex"><img src="images/dplyr.png" class="title-hex">

Next, we will attach the predictions for each resample:

.pull-left[

```{r, compute-pred}
compute_pred <- function(split, model) {
  
  # Extract the assessment set
  assess <- assessment(split) %>%
    mutate(Sale_Price_Log = log10(Sale_Price))
  
  # Compute predictions (a df is returned)
  pred <- predict(model, new_data = assess)
  
  bind_cols(assess, pred)
}
```

]

.pull-right[

```{r purrr-lm-pred}
cv_splits <- cv_splits %>%
  mutate(
    pred_lm = map2(splits, models_lm, compute_pred)
  )
cv_splits
```

]


---

# Resampling the Linear Model <img src="images/purrr.png" class="title-hex"><img src="images/parsnip.png" class="title-hex"><img src="images/dplyr.png" class="title-hex">

Now, let's compute two performance measures:

.pull-left[

```{r purrr-lm-results}
compute_perf <- function(pred_df) {
  
  # Create a function that calculates
  # rmse and rsq and returns a data frame
  numeric_metrics <- metric_set(rmse, rsq)
  
  numeric_metrics(
    pred_df, 
    truth = Sale_Price_Log, 
    estimate = .pred
  )
  
}
```

]

.pull-right[
```{r purrr-lm-estimate}
cv_splits <- cv_splits %>%
  mutate(perf_lm = map(pred_lm, compute_perf))

select(cv_splits, pred_lm, perf_lm)
```
]

---

# Resampling the Linear Model <img src="images/purrr.png" class="title-hex"><img src="images/parsnip.png" class="title-hex"><img src="images/dplyr.png" class="title-hex">

And finally, let's compute the average of each metric over the resamples:

.pull-left[

```{r}
cv_splits$perf_lm[[1]]
```

]

.pull-right[

```{r}
cv_splits %>%
  unnest(perf_lm) %>%
  group_by(.metric) %>%
  summarise(
    .avg = mean(.estimate),
    .sd = sd(.estimate)
  )
```

]

---

# What Was the Ruckus?

```{r, include=FALSE}
rmse_cv <- cv_splits$perf_lm %>%
  bind_rows() %>%
  filter(.metric == "rmse") %>%
  pull(.estimate) %>%
  mean()
```


Previously, I mentioned that the performance metrics that were naively calculated from the training set could be optimistic. However, this approach estimates the RMSE to be `r round(summary(simple_lm)$sigma, 4)` and cross-validation produced an estimate of `r round(rmse_cv, 4)`. What was the big deal? 

Linear regression is a _high bias model_. This means that it is fairly incapable at being able to adapt the underlying model function (unless it is linear). For this reason, linear regression is unlikely to **overfit** to the training set and our two estimates are likely to be the same. 

We'll consider another model shortly that is _low bias_ since it can, theoretically, easily adapt to a wide variety of true model functions. 

However, as before, there is also variance to consider. Linear regression is very stable since it leverages all of the data points to estimate parameters. Other methods, such as tree-based models, are not and can drastically change if the training set data is slightly perturbed. 

**tl;dr**: the earlier concern is real but linear regression is less likely to be affected.  

---

# Diagnostics Again <img src="images/rsample.png" class="title-hex"><img src="images/purrr.png" class="title-hex"><img src="images/broom.png" class="title-hex">

Now let's look at diagnostics using the predictions from the assessment sets. 

```{r lm-assess, warning=FALSE, message=FALSE}
holdout_results <- 
  cv_splits %>%
  unnest(pred_lm) %>%
  mutate(.resid = Sale_Price_Log - .pred)

holdout_results %>% dim()
ames_train %>% dim()
```

---

# Hands-On: Partial Residual Plots <img src="images/rsample.png" class="title-hex"><img src="images/purrr.png" class="title-hex"><img src="images/broom.png" class="title-hex">

A partial residual plot is used to diagnose what variables _should_ have been in the model. 

We can plot the hold-out residuals versus different variables to understand if they should have been in the model

 * If the residuals have no pattern in the data, they are likely to be irrelevant. 
 
 * If a pattern is seen, it suggests that the variable should have been in the model.
 
Take 10 min and use `ggplot` to investigate the other predictors using the `holdout_results` data frame. `geom_smooth()` might come in handy. 


---
layout: false
class: inverse, middle, center

#  Tuning Parameters and Overfitting


---

# _K_-Nearest Neighbors Model

Now let's consider a more flexible model that is _low bias_: _K_-nearest neighbors.

The model stores the training set (including the outcome). 

When a new sample is predicted, _K_ training set points are found that are most similar to the new sample being predicted. 

The predicted value for the new sample is some summary statistic of the neighbors, usually: 

* the mean for regression, or
* the mode for classification.

When _K_ is small, the model might be _too_ responsive to the underlying data. When _K_ is large, it begins to "over smooth" the neighbors and performance suffers. 

Ordinarily, since we are computing a distance, we would want to center and scale the predictors. Our two predictors are already on the same scale so we can skip this step. 

---

# _5_-Nearest Neighbors Model

```{r knn-map, echo = FALSE, message = FALSE, fig.align='center'}
pp <- preProcess(ames_train[, c("Latitude", "Longitude")])
train_ll <- predict(pp, ames_train[, c("Latitude", "Longitude")])
test_ll  <- predict(pp, ames_test[, c("Latitude", "Longitude")])

test_instance <- 13
k <- 5
dists <- proxy::dist(as.matrix(train_ll),
                     as.matrix(test_ll[test_instance, , drop = FALSE]))
dists <- data.frame(distance = dists[, 1],
                    row = 1:nrow(dists))
dists <- head(dists[order(dists),], k)

local_ames_train <- ames_train[dists$row,]

local_ames_train$errors <- unlist(rep(ames_test[test_instance, "Sale_Price"], k)) - 
  local_ames_train$Sale_Price
local_ames_train$to_log <- unlist(rep(ames_test[test_instance, "Longitude"], k))
local_ames_train$to_lat <- unlist(rep(ames_test[test_instance, "Latitude"], k))

knn_map <- leaflet(width = "80%") %>%
  setView(lng = local_ames_train$to_log[1], lat = local_ames_train$to_lat[1], zoom = 18) %>%
  addProviderTiles(providers$CartoDB.DarkMatterNoLabels) %>%
  addCircles(
    data = local_ames_train,
    lng = ~Longitude, lat = ~Latitude,
    color = "red",
    fill = TRUE,
    fillColor = "red",
    radius = 6,
    opacity = .4) %>%
  addCircles(
    data = local_ames_train[1,],
    lng = ~to_log, lat = ~to_lat,
    color = "yellow",
    fill = TRUE,
    fillColor = "yellow",
    radius = 6,
    opacity = .4) %>%
  addCircles(
    data = ames_train[-dists$row,],
    lng = ~Longitude, lat = ~Latitude,
    color = "white",
    fill = TRUE,
    fillColor = "white",
    radius = 3,
    opacity = .4) 

for (i in 1:nrow(local_ames_train)) {
  knn_map <- knn_map %>% 
    addPolylines(lat = as.numeric(local_ames_train[i, c("to_lat", "Latitude")]), 
                 lng = as.numeric(local_ames_train[i, c("to_log", "Longitude")]),
                 color = "yellow",
                 weight = 1)
}

knn_map
```


---

# _K_-Nearest Neighbors Model <img src="images/parsnip.png" class="title-hex"><img src="images/dplyr.png" class="title-hex">

Consider the 2-nearest neighbor model. Would there be a difference in the estimated model performance between re-prediction and cross-validation? 

`parsnip` has a `nearest_neighbors()` specification that uses the `kknn` package. `K` is standardized to the name `neighbors` and we will use that going forward.

.pull-left[

```{r spec-knn}
spec_knn <- nearest_neighbor(neighbors = 2) %>%
  set_engine("kknn")

spec_knn

fit_knn <- fit(spec_knn, geo_form, ames_train_log)
```

]

.pull-right[

```{r printed-fit-knn}
fit_knn
```

]


---

# _K_-Nearest Neighbors Model <img src="images/parsnip.png" class="title-hex"><img src="images/dplyr.png" class="title-hex">

.pull-left[

```{r repredicted-knn}
# Predict on the same data you train with
repredicted <- fit_knn %>%
  predict(new_data = ames_train_log) %>%
  bind_cols(ames_train_log) %>%
  dplyr::select(.pred, Sale_Price_Log)

repredicted
```

]

.pull-right[

```{r knn-ruckus}
# The ruckus is here!
repredicted %>% 
  rsq(
    truth = Sale_Price_Log, 
    estimate = .pred
  )
```

]


---
# Resampling a 2-Nearest Neighbor Model <img src="images/rsample.png" class="title-hex"><img src="images/purrr.png" class="title-hex"><img src="images/parsnip.png" class="title-hex"><img src="images/dplyr.png" class="title-hex">

That's pretty good but are we tricking ourselves? One of those two neighbors is always itself...

Let's follow the same resampling process as before, reusing some of our other
functions for generating the models, predictions, and performance metrics:

.pull-left[

```{r cv-with-knn}
cv_splits <- cv_splits %>%
  mutate(
    # Fit a knn model for each split
    models_knn = map(splits, fit_model, spec_knn),
    
    # Generate predictions on the assessment set
    pred_knn = map2(splits, models_knn, compute_pred),
    
    # Calculation performance
    perf_knn = map(pred_knn, compute_perf)
  )
```

]

.pull-right[

```{r}
# Unnest & compute resampled performance estimates
cv_splits %>%
  unnest(perf_knn) %>%
  group_by(.metric) %>%
  summarise(
    .estimate_mean = mean(.estimate),
    .estimate_sd = sd(.estimate)
  )
```

]

---

# Making Formal Comparisons

```{r, include=FALSE}
extract_rmse <- function(perf_list) {
  perf_list %>%
    bind_rows() %>%
    filter(.metric == "rmse") %>%
    pull(.estimate)
}

rmse_lm <- extract_rmse(cv_splits$perf_lm)
rmse_knn <- extract_rmse(cv_splits$perf_knn)
```


The model appears to be a drastic improvement over simple linear regression but we are definitely getting highly optimistic results by re-predicting the training set. 

We can try to make a more formal assessment of the two current models. 

Both models used the _same_ resamples, so we have 10 estimates of performance that are matched.

Does the matching mean anything? 

Most likely **yes**. It is very common to see that there is a resample effect. Similar to repeated measures designs, we can expect a relationship between models and resamples. For example, some resamples will have the worst performance over different models and so on. 

In other words, there is usually a within-resample correlation. For the two models, the estimated correlation in RMSE values is `r round(cor(rmse_lm ,rmse_knn), 3)`. 


---

# The Resample Effect

```{r cv-corr, echo = FALSE, fig.width=6, fig.height=4.25,  out.width = '60%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}
rs_comp <- data.frame(
	rmse = c(rmse_lm, rmse_knn),
	Model = rep(c("Linear\nRegression", "2-NN"), each = nrow(cv_splits)),
	Resample = cv_splits$id
)

ggplot(rs_comp, aes(x = Model, y = rmse, group = Resample, col = Resample)) + 
  geom_point() + 
  geom_line() + 
  theme(legend.position = "none")
```

---

# Model Comparison Accounting for Resampling

With only two models, a paired _t_-test can be used to estimate the difference in RMSE between the models: 

```{r cv-comp}
t.test(rmse_lm, rmse_knn, paired = TRUE)
```


Hothorn _et al_ (2012) is the [original paper](https://scholar.google.com/scholar?hl=en&q=analysis+of+benchmark+experiments&btnG=&as_sdt=1%2C7&as_sdtp=) on comparing models using resampling. 

We'll do more extensive analyses with `tidyposterior` soon. 


---

# Overfitting

Overfitting occurs when a model inappropriately picks up on trends in the training set that do not generalize to new samples. 

When this occurs, assessments of the model based on the training set can show good performance that does not reproduce in future samples.   
  
Some models have specific "knobs" to control over-fitting

* neighborhood size in nearest neighbor models is an example

* the number of splits in a tree model

Often, poor choices for these parameters can result in overfitting

For example, the next slide shows a data set with two predictors. We want to be able to produce a line (i.e. decision boundary) that differentiates two classes of data.

---

# Two Class Example

.pull-left[
```{r two-class-2panel, echo = FALSE, fig.width=5, fig.height=5.25,  out.width = '90%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}
set.seed(14034)
two_class_dat <- easyBoundaryFunc(250, intercept = -6, interaction = 1.5)
rng_1 <- extendrange(two_class_dat$X1)
rng_2 <- extendrange(two_class_dat$X2)

set.seed(1)
contour_grid <- expand.grid(X1 = seq(rng_1[1], rng_1[2], length = 200),
                            X2 = seq(rng_2[1], rng_2[2], length = 200))

two_class_ctrl <- trainControl(method = "cv", classProbs = TRUE,
                               summaryFunction = twoClassSummary)
two_class_ctrl_rand <- trainControl(method = "cv", classProbs = TRUE,
                                    search = "random", 
                                    summaryFunction = twoClassSummary)
set.seed(235)
two_class_mod_1 <- train(class ~ X1 + X2,
                         data = two_class_dat,
                         method = "nnet",
                         tuneLength = 20,
                         trace = FALSE,
                         metric = "ROC",
                         trControl = two_class_ctrl_rand)

two_class_prob_1 <- contour_grid
two_class_prob_1$prob <- predict(two_class_mod_1, contour_grid, type ="prob")[,1]
two_class_prob_1$Model = "Model #1"

set.seed(235)
two_class_mod_2 <- train(class ~ X1 + X2,
                         data = two_class_dat,
                         method = "nnet",
                         tuneGrid = data.frame(decay = 0.003, size = 25),
                         trace = FALSE,
                         metric = "ROC",
                         trControl = two_class_ctrl)

two_class_prob_2 <- contour_grid
two_class_prob_2$prob <- predict(two_class_mod_2, contour_grid, type ="prob")[,1]
two_class_prob_2$Model = "Model #2"

both <- rbind(two_class_prob_1, two_class_prob_2)

ggplot(two_class_dat, aes(x = X1, y = X2)) +
  geom_point(aes(color = class, shape = class), cex = 3, alpha = .5) +
  guides(colour = guide_legend(""), shape = guide_legend("")) +
  xlab("Predictor A") + ylab("Predictor B")  +
  scale_colour_tableau() + 
  theme(legend.position = "top")
```

]

.pull-right[

On the next slide, two classification boundaries are shown for a different model type not yet discussed.

The difference in the two panels is solely due to different choices in tuning parameters. 

One overfits the training data.

]


---

# Two Model Fits

```{r two-class-overfit, echo = FALSE, fig.width=7.5, fig.height=4.25,  out.width = '70%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}
ggplot(two_class_dat, aes(x = X1, y = X2)) +
  geom_point(aes(color = class, shape = class), cex = 2, alpha = .5) +
  facet_wrap(~Model) + 
  geom_contour(data = both, aes(z = prob), breaks = .5, col = "black", lwd = 1) + 
  guides(colour = guide_legend(""), shape = guide_legend("")) +
  xlab("Predictor A") + ylab("Predictor B")  +
  theme(legend.position = "top") + 
  scale_colour_tableau()
```


---

# Grid Search to Tune Models

We usually don't have two-dimensional data so a quantitative method for under measuring overfitting is needed.  _Resampling_ fits that description. A simple method for tuning a model is to used _grid search_:

<pre>
├── Create a set of candidate tuning parameter values
└── For each resample
│   ├── Split the data into analysis and assessment sets
│   ├── [preprocess data]
│   ├── For each tuning parameter value
│   │   ├── Fit the model using the analysis set
│   │   └── Compute the performance on the assessment set and save 
├── For each tuning parameter value, average the performance over resamples
├── Determine the best tuning parameter value
└── Create the final model with the optimal parameter(s) on the training set
</pre>

_Random search_ is a similar technique where the candidate set of parameter values are simulated at random across a wide range. Also, an example of _nested resampling_ can be found [here](http://appliedpredictivemodeling.com/blog/2017/9/2/njdc83d01pzysvvlgik02t5qnaljnd). 

---

# Grid Search Computations

* **The bad news**

  All of the models (except the final model) are discarded. 

* **The good news** 
  
  All of the models (except the final model) can be run in parallel. 

Let's look at the Ames K-NN model and evaluate _neighbors_ = 1, 2, ..., 20 using the same 10-fold cross-validation as before. 

---

# dials

To tune `parsnip` models, two concepts are important to understand:

1) `varying()` parameters are those that you want to tune over.

2) `dials` is a package for filling in those varying parameters with a tuning grid.

.pull-left[

```{r dials-intro}
# Parameter object for `neighbors`
neighbors

# Number of neighbors varies from 1-20
param_grid <- 
  neighbors %>% 
  range_set(c(1, 20)) %>%
  grid_regular(levels = 20)
```

]

.pull-right[

```{r}
glimpse(param_grid)

# Declare `neighbors` as varying
spec_knn_varying <- nearest_neighbor(
    neighbors = varying()
  ) %>%
  set_engine("kknn") %>% 
  set_mode("regression")  # not required
```

]

???

Note that we don't have to set the mode but it prevents the specifications 
from printing in the tibble as `<spec[?]>` instead of `<spec[+]>`

---

# dials

`merge()` can be used join a parameter grid to a specification. This fills in
the `neighbors` field of the specification with the 20 different values.

.pull-left[

```{r dials-merge}
param_grid <- 
  param_grid %>%
  mutate(
    specs = merge(., spec_knn_varying)
  )

print(param_grid, n = 4)
```

]

.pull-right[

```{r dials-one-finalized-spec}
param_grid$specs[[20]]
```

Now that we have the specification grid, we will start coding this algorithm from the inside out...

]

???
``<spec[+]>` means that the mode is set and all parameters are known. 


---

# A One Split + One Spec Combo <img src="images/rsample.png" class="title-hex"><img src="images/parsnip.png" class="title-hex"><img src="images/dplyr.png" class="title-hex">  

These steps are:

<pre>
   ├── Fit the model using the analysis set
   └── Compute the performance on the assessment set and save 
</pre>

In the code below, `split` will be one of the elements of `cv_splits$splits` and
`spec` is one of `param_grid$specs`. We can reuse many of the functions we've
already created.

.pull-left[

```{r grid-search-perf}
fit_one_spec_one_split <- function(spec, split) {
  mod <- fit_model(split, spec)
  pred_df <- compute_pred(split, mod)
  perf_df <- compute_perf(pred_df)
  
  # pull out only rmse
  perf_df %>%
    filter(.metric == "rmse") %>%
    pull(.estimate)
}
```

]

.pull-right[

```{r, single-spec-single-split}
fit_one_spec_one_split(
  param_grid$specs[[6]],  # Six neighbors
  cv_splits$splits[[9]]  # Ninth Fold
)
```

]

---

# One Split + All Specs <img src="images/purrr.png" class="title-hex"><img src="images/parsnip.png" class="title-hex">

Now we apply `fit_one_spec_one_split()` to every parameter combination.

<pre>
│   ├── For each tuning parameter value
│   │   └── Run `fit_one_spec_one_split()`
</pre>

.pull-left[

```{r grid-search-grid}
fit_all_specs_one_split <- function(split, param_df) {
  param_df %>%
    mutate(
      rmse = map_dbl(
        specs, 
        fit_one_spec_one_split, 
        split = split
      )
    )
}
```

]

.pull-right[

```{r all-spec-single-split}
fit_all_specs_one_split(
  cv_splits$splits[[1]], 
  param_grid
) %>%
  print(n = 5)
```

]

---

# All Splits + All Specs <img src="images/purrr.png" class="title-hex"><img src="images/parsnip.png" class="title-hex">

<pre>
└── For each resample
│   └── Run `fit_all_specs_one_split()`
</pre>

Here, `split_df` is the resample object `cv_splits` and `param_df` is `param_grid`.

```{r grid-search-top}
fit_all_specs_all_splits <- function(split_df, param_df) {
  split_df %>%
    mutate(
      spec_perf = map(
        splits, 
        fit_all_specs_one_split, 
        param_df = param_df
      )
    ) %>%
    dplyr::select(splits, id, spec_perf)
}
```

This outputs a tibble with columns for the resample, the resample id (`"Fold01"`), and a
list column of the performance of each tuning parameter combination for that resample.

---

# Running the Code

.pull-left[

```{r tune-knn}
resampled_grid <- fit_all_specs_all_splits(
  split_df = cv_splits, 
  param_df = param_grid
)

resampled_grid %>% slice(1:6)
```

]

.pull-right[

```{r unnested-tune}
# Keep the unnested version
unnested_grid <- 
  resampled_grid %>%
  unnest(spec_perf) %>%
  dplyr::select(-specs)

unnested_grid %>% slice(1:6)
```

]

---

# The Performance Profile <img src="images/ggplot2.png" class="title-hex"><img src="images/dplyr.png" class="title-hex">

.pull-left[

To summarize the results for each value of `neighbors`: 

```{r aummarize-knn, eval = FALSE}
rmse_by_neighbors <- 
  unnested_grid %>%
  group_by(neighbors) %>%
  summarize(rmse = mean(rmse))

ggplot(
    rmse_by_neighbors, 
    aes(x = neighbors, y = rmse)
  ) + 
  geom_point() + 
  geom_line()
```
]

.pull-right[
```{r ames-knn, echo = FALSE, fig.width=6.5, fig.height=4.25,  out.width = '100%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}
rmse_by_neighbors <- 
  unnested_grid %>%
  group_by(neighbors) %>%
  summarize(rmse = mean(rmse))

ggplot(
    rmse_by_neighbors, 
    aes(x = neighbors, y = rmse)
  ) + 
  geom_point() + 
  geom_line()
```

Although it is numerically optimal, we are not required to use a value of `r rmse_by_neighbors$neighbors[which.min(rmse_by_neighbors$rmse)]` neighbors for the final model.

]

---

# Resampling Variation <img src="images/ggplot2.png" class="title-hex"><img src="images/dplyr.png" class="title-hex">


.pull-left[

How stable is this? We can also plot the individual curves and their minimums.

```{r summarize-best, eval = FALSE}
best_neighbors <- 
  unnested_grid %>%
  group_by(id) %>%
  summarize(neighbors = neighbors[which.min(rmse)],
            rmse      = rmse[which.min(rmse)])

ggplot(rmse_by_neighbors, 
       aes(x = neighbors, y = rmse)) + 
  geom_point() + 
  geom_line() + 
  geom_line(data = unnested_grid, 
            aes(group = id, col = id),
            alpha = .2, lwd = 1) + 
  geom_point(data = best_neighbors, 
             aes(col = id),
             alpha = .5, cex = 2) +
  theme(legend.position = "none")
```
]
.pull-right[
```{r ames-knn-indiv, echo = FALSE, fig.width=6.5, fig.height=4.25,  out.width = '100%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}
best_neighbors <- 
  unnested_grid %>%
  group_by(id) %>%
  summarize(neighbors = neighbors[which.min(rmse)],
            rmse      = rmse[which.min(rmse)])

ggplot(rmse_by_neighbors, 
       aes(x = neighbors, y = rmse)) + 
  geom_point() + 
  geom_line() + 
  geom_line(data = unnested_grid, 
            aes(group = id, col = id),
            alpha = .2, lwd = 1) + 
  geom_point(data = best_neighbors, 
             aes(col = id),
             alpha = .5, cex = 2) +
  theme(legend.position = "none")
```

]

---

# Next Steps <img src="images/parsnip.png" class="title-hex">

At this point, we would decide on a good value for _neighbors_, fit that model, and use it going forward:

.pull-left[

```{r knn-final-value, eval = FALSE}
best_neighbor_value <- 
  rmse_by_neighbors %>%
  filter(rmse == min(rmse)) %>%
  pull(neighbors)

best_spec <- 
  param_grid %>%
  filter(neighbors == best_neighbor_value) %>%
  pull(specs) %>%
  .[[1]]
```

]

.pull-right[

```{r knn-final, eval=FALSE}
fit(
  best_spec,
  geo_form,
  ames_train
)
```

]


To reiterate: the previous `r nrow(resampled_grid)` models created during the grid search are not used once _neighbors_ is set. 

Later, we will look at a high-level API in `caret` that streamlines almost all of this process for many different models. A similar process is being created for the modular tidy packages you see here. We'll talk about this later. 