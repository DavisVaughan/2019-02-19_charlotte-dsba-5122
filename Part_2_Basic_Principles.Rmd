---
title: "Modeling Principles - Fitting and Resampling"
author: Davis Vaughan (RStudio) (and Max Kuhn)
output:
  xaringan::moon_reader:
    css: ["mtheme_max.css", "fonts_mtheme_max.css"]  
    self_contained: false
    nature:
      ratio: '16:9'
      highlightLanguage: R
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

# Introduction

```{r startup, include = FALSE, message = FALSE, warning = FALSE}
library(caret)
library(ggplot2)
library(leaflet)
library(AppliedPredictiveModeling)
library(ggthemes)
library(DiagrammeR)

thm <- theme_bw() + 
  theme(
    panel.background = element_rect(fill = "transparent", colour = NA), 
    plot.background = element_rect(fill = "transparent", colour = NA),
    legend.position = "top",
    legend.background = element_rect(fill = "transparent", colour = NA),
    legend.key = element_rect(fill = "transparent", colour = NA)
  )
theme_set(thm)
```

In this presentation, we will cover a number of topics that transfer to any machine learning model:

* `purrr::map()` and list columns

* Interfaces for fitting models in R (formula, xy, parsnip)

* Data splitting (training and test sets)

* Resampling (cross validation, bootstraps)

* Fitting multiple models (purrr + rsample + parsnip = `r emo::ji("heart")`)

We will use a simple linear regression model to demonstrate these concepts.

_The point is not to create the best model, but to demonstrate tooling and best practices!_

---

# Data Set - House Prices

We will use the Ames IA housing data. There are 2,930 properties in the data. 

The Sale Price was recorded along with 81 predictors, including:

* Location (e.g. neighborhood) and lot information.
* House components (garage, fireplace, pool, porch, etc.).
* General assessments such as overall quality and condition.
* Number of bedrooms, baths, and so on. 

More details can be found in [De Cock (2011, Journal of Statistics Education)](http://ww2.amstat.org/publications/jse/v19n3/decock.pdf).

The raw data are at [`http://bit.ly/2whgsQM`](http://bit.ly/2whgsQM) but we will use a processed version found in the [`AmesHousing`](https://github.com/topepo/AmesHousing) package. 

```{r}
library(AmesHousing)
ames <- make_ames()
```

---

# Data Set - House Prices

```{r ames-map, echo = FALSE, message = FALSE, fig.align='center', dev = "svg", warning=FALSE}
library(leaflet)
library(htmltools)
library(Cairo)
library(dplyr)

col_key <- c(
  'NAmes',     '#0000FF',
  'CollgCr',   '#FF0000',
  'OldTown',   '#FFFFFF',
  'Edwards',   '#FF00B6',
  'Somerst',   '#FF3030',
  'NridgHt',   '#009FFF',
  'Gilbert',   '#DD00FF',
  'Sawyer',    '#9A4D42',
  'NWAmes',    '#00FFBE',
  'SawyerW',   '#1F9698',
  'Mitchel',   '#FFACFD',
  'BrkSide',   '#720055',
  'Crawfor',   '#F1085C',
  'IDOTRR',    '#FE8F42',
  'Timber',    '#004CFF',
  'NoRidge',   '#ffff00',
  'StoneBr',   '#B1CC71',
  'SWISU',     '#02AD24',
  'ClearCr',   '#FFD300',
  'MeadowV',   '#886C00',
  'BrDale',    '#FFB79F',
  'Blmngtn',   '#858567',
  'Veenker',   '#A10300',
  'NPkVill',   '#00479E',
  'Blueste',   '#DC5E93',
  'Greens',    '#93D4FF',
  'GreenHills', '#e5f2e5', 
  'Landmrk',   '#C8FF00'
) 
col_key <- as.data.frame(matrix(col_key, byrow = TRUE, ncol = 2))
names(col_key) <- c("Neighborhood", "color")
col_key <- col_key %>%
    mutate(
      Neighborhood =
        dplyr::recode(
          Neighborhood,
          "Blmngtn" = "Bloomington_Heights",
          "Bluestem" = "Bluestem",
          "BrDale" = "Briardale",
          "BrkSide" = "Brookside",
          "ClearCr" = "Clear_Creek",
          "CollgCr" = "College_Creek",
          "Crawfor" = "Crawford",
          "Edwards" = "Edwards",
          "Gilbert" = "Gilbert",
          "Greens" = "Greens",
          "GreenHills" = "Green_Hills",
          "IDOTRR" = "Iowa_DOT_and_Rail_Road",
          "Landmrk" = "Landmark",
          "MeadowV" = "Meadow_Village",
          "Mitchel" = "Mitchell",
          "NAmes" = "North_Ames",
          "NoRidge" = "Northridge",
          "NPkVill" = "Northpark_Villa",
          "NridgHt" = "Northridge_Heights",
          "NWAmes" = "Northwest_Ames",
          "OldTown" = "Old_Town",
          "SWISU" = "South_and_West_of_Iowa_State_University",
          "Sawyer" = "Sawyer",
          "SawyerW" = "Sawyer_West",
          "Somerst" = "Somerset",
          "StoneBr" = "Stone_Brook",
          "Timber" = "Timberland",
          "Veenker" = "Veenker"
        ))

lon_rnd <- range(ames$Longitude)
lat_rnd <- range(ames$Latitude)

ia_map <- leaflet(width = "100%") %>%
  addProviderTiles(providers$Stamen.Toner)

for(i in 1:nrow(col_key)) {
  ia_map <- ia_map %>%
    addCircles(
      data = subset(ames, Neighborhood == col_key$Neighborhood[i]),
      lng = ~Longitude, lat = ~Latitude,
      color = col_key$color[i],
      fill = TRUE,
      fillColor = col_key$color[i],
      radius = 6,
      popup = htmlEscape(col_key$Neighborhood[i]),
      opacity = .25)
}
ia_map
```

---

# `tidymodels` <img src="images/tidymodels_hex.png" class="title-hex">


```{r tm}
library(tidymodels)
```

Plus [`tidypredict`](http://tidypredict.netlify.com/), [`tidyposterior`](https://tidymodels.github.io/tidyposterior/), [`tidytext`](https://github.com/juliasilge/tidytext), and more in development.

---

# Examples of `purrr::map()` <img src="images/dplyr.png" class="title-hex"><img src="images/purrr.png" class="title-hex">

```{r purrr-setup, message = FALSE, echo=FALSE}
library(tidyverse)
```

purrr contains functions that _iterate over lists_ without the explicit use of loops. They are similar to the family of apply functions in base R, but are type stable.

.pull-left[

```{r purrr-example-lhs}
library(purrr)

mini_ames <- ames %>%
  select(Alley, Sale_Price, Year_Sold) %>%
  filter(Alley != "No_Alley_Access") %>%
  mutate(Alley = fct_drop(Alley))

head(mini_ames, n = 5)
```

]

.pull-right[
```{r purrr-split-map}
by_alley <- split(mini_ames, mini_ames$Alley)
map(by_alley, head, n = 2)
```

]

---

# Examples of `purrr::map()` <img src="images/dplyr.png" class="title-hex"><img src="images/purrr.png" class="title-hex">

.pull-left[
```{r purrr-map-nrow}
map(by_alley, nrow)
```

`map()` always returns a list. Use suffixed versions for simplification of the result.

```{r purrr-map-int}
map_int(by_alley, nrow)
```

]

.pull-right[

Complex operations can be specified using a _formula notation_. Access the current thing you are iterating over with `.x`.

```{r purrr-map-summarise}
map(
  by_alley, 
  ~summarise(.x, max_price = max(Sale_Price))
)
```

]

---

# `purrr` and list-columns <img src="images/dplyr.png" class="title-hex"><img src="images/purrr.png" class="title-hex"><img src="images/tidyr.png" class="title-hex">

Rather than using `split()`, we can `tidyr::nest()` by `Alley` to get a data frame with
a _list-column_. We often use these when working with _multiple models_.

.pull-left[

```{r tidyr-nest}
ames_lst_col <- nest(mini_ames, -Alley)
ames_lst_col
```

]

.pull-right[

```{r list-col-mutate}
ames_lst_col <- ames_lst_col %>%
  mutate(
    model = map(
      .x = data,
      .f =  ~lm(Sale_Price ~ Year_Sold, data = .x)
    ),
    perf  = map(model, broom::tidy)
  )

ames_lst_col
```

]

---

# List-columns and `unnest()` <img src="images/purrr.png" class="title-hex"><img src="images/tidyr.png" class="title-hex">

`unnest()` repeats regular columns once for each row of the unnested list-column. `"Paved"` is repeated 2 times, and `"Gravel"` is repeated 2 times.

.pull-left[

```{r list-col-print}
ames_lst_col
```

You can unnest multiple list-columns at once if they have the same number of rows. We will use this when unnesting predictions for each resample.

]

.pull-right[

```{r unnest-example}
unnest(ames_lst_col, perf)
```

]

---
layout: false
class: inverse, middle, center

# Modeling

---

# The Modeling _Process_

Common steps during model building are:

* estimating model parameters (i.e. training models)

* computing resampled performance estimates to compare models with

* determining the values of _tuning parameters_ that cannot be directly calculated from the data

* model selection (within a model type) and model comparison (between types)

* calculating the performance of the final model that will generalize to new data

Many books and courses portray predictive modeling as a short sprint. A better analogy would be a marathon or campaign (depending on how hard the problem is). 


---

# What the Modeling Process Usually Looks Like

```{r mod-process, echo = FALSE, out.width = '95%', fig.width=8, fig.height=2.5, fig.align='center', dev = 'svg', dev.args = list(bg = "transparent"), warning=FALSE}
widths <- c(8, 4, 10, 2, 6, 6, 
            rep(1, 19), 2,
            rep(1, 19), 2,
            rep(1, 19), 2,
            rep(1, 19), 2,
            4, 8, 15,
            rep(1, 29), 6,
            rep(1, 29), 4,
            1)
srt <- c(1, cumsum(widths))
stp <- srt[-1]
srt <- srt[-length(srt)]

diag_cols <- c(EDA = "#377EB8", "Quantitative Analysis" = "#A6CEE3", 
               "Feature Engineering" = "#4DAF4A", "Model Fit" = "#E41A1C", 
               "Model Tuning" = "grey")

bar_loc <- data.frame(srt = srt,
                  stp = stp,
                  g = c("EDA", "Quantitative Analysis", "EDA", "Quantitative Analysis", "EDA", "Feature Engineering", 
                        rep(c("Model Fit", "Model Tuning"), 40),
                        "Quantitative Analysis", "EDA", "Feature Engineering",
                        rep(c("Model Fit", "Model Tuning"), 14), "Model Fit", "Feature Engineering",
                        rep(c("Model Fit", "Model Tuning"), 14), "Model Fit", "Quantitative Analysis",
                        "Model Fit"))
bar_loc$ytop <- 1.9
bar_loc$ybot <- 1
bar_loc$g <- factor(as.character(bar_loc$g), 
                levels = c("EDA", "Quantitative Analysis", "Feature Engineering",
                           "Model Fit", "Model Tuning"))
text_loc <- data.frame(x = c(1, 8, 30, 36, 120, 124, 132, 147, 211, 215)+1,
                       y = 2.1)
text_loc$label <- letters[1:nrow(text_loc)]

mod_loc <- data.frame(x = c(45, 66, 87, 107, 162, 195)+1,
                      y = .75, 
                      label = c("Model\n#1", "Model\n#2", "Model\n#3", "Model\n#4",
                                "Model\n#2", "Model\n#4"))

ggplot(bar_loc) + 
  geom_rect(aes(fill = g, xmin = srt, xmax = stp,
                ymin = ybot, ymax = ytop), alpha = .7)  + 
  theme(
    legend.position = "bottom",
    legend.background = element_blank(),
    axis.line = element_blank(),
    axis.text.x = element_blank(),
    axis.text.y = element_blank(),
    axis.ticks = element_blank(),
    axis.title.x = element_text(hjust = .05),
    axis.title.y = element_blank(),
    panel.background = element_blank(),
    panel.border = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    plot.background = element_blank()
    ) +
  scale_fill_manual(values = diag_cols, name = "") +
  geom_text(data = text_loc, aes(x = x, y = y, label = label)) + 
  geom_text(data = mod_loc, aes(x = x, y = y, label = label), size = 3) +   
  xlab("Time") + 
  ylim(c(.5, 2.25))
```

---
layout: false
class: inverse, middle, center

#  Data Usage

---

# Data Splitting and Spending

How do we "spend" the data to find an optimal model? 

We _typically_ split data into training and test data sets:

*  ***Training Set***: these data are used to estimate model parameters and to pick the values of the complexity parameter(s) for the model.

*  ***Test Set***: these data can be used to get an independent assessment of model efficacy. They should not be used during model training. 


---

# Data Splitting and Spending 

The more data we spend, the better estimates we'll get (provided the data is accurate).  

Given a fixed amount of data:

* too much spent in training won't allow us to get a good assessment of predictive performance.  We may find a model that fits the training data very well, but is not generalizable (overfitting)

* too much spent in testing won't allow us to get a good assessment of model parameters

Statistically, the best course of action would be to use all the data for model building and use statistical methods to get good estimates of error.

From a non-statistical perspective, many consumers of complex models emphasize the need for an untouched set of samples to evaluate performance.

---

# Mechanics of Data Splitting

There are a few different ways to do the split: simple random sampling, _stratified sampling based on the outcome_, by date, or methods that focus on the distribution of the predictors.

For stratification:

* **classification**: this would mean sampling within the classes to preserve the distribution of the outcome in the training and test sets

* **regression**: determine the quartiles of the data set and sample within those artificial groups


---

# Ames Housing Data <img src="images/rsample.png" class="title-hex">

Let's load the example data set and split it. We'll put 75% into training and 25% into testing. 

```{r ames-split, message = FALSE}
library(AmesHousing)

# Remove quality-related predictors
ames <- dplyr::select(ames, -matches("Qu"))

nrow(ames)

# Make sure that you get the same random numbers
set.seed(4595)
data_split <- initial_split(ames, strata = "Sale_Price")

ames_train <- training(data_split)
ames_test  <- testing(data_split)

nrow(ames_train) / nrow(ames)
```
???

The select statement removes subjective quality scores which, to me, seems
like it should be an outcome and not a predictor. 

---

# Ames Housing Data <img src="images/rsample.png" class="title-hex">

What do these objects look like?

```{r}
# result of initial_split()
# <training / testing / total>
data_split
```

```{r, eval=FALSE}
training(data_split)
```

```{r}
## # A tibble: 2,199 x 81
##    MS_SubClass MS_Zoning Lot_Frontage Lot_Area Street Alley Lot_Shape Land_Contour Utilities Lot_Config Land_Slope
##    <fct>       <fct>            <dbl>    <int> <fct>  <fct> <fct>     <fct>        <fct>     <fct>      <fct>     
##  1 One_Story_… Resident…          141    31770 Pave   No_A… Slightly… Lvl          AllPub    Corner     Gtl       
##  2 Two_Story_… Resident…           74    13830 Pave   No_A… Slightly… Lvl          AllPub    Inside     Gtl       
##  3 Two_Story_… Resident…           78     9978 Pave   No_A… Slightly… Lvl          AllPub    Inside     Gtl       
##  4 One_Story_… Resident…           43     5005 Pave   No_A… Slightly… HLS          AllPub    Inside     Gtl       
##  5 One_Story_… Resident…           39     5389 Pave   No_A… Slightly… Lvl          AllPub    Inside     Gtl       
## # … and many more rows and columns
## # …
```


---

# Outcome Distributions <img src="images/ggplot2.png" class="title-hex">

.pull-left[

```{r ames-split-dists, eval = FALSE}
## Do the distributions line up? 
ggplot(ames_train, aes(x = Sale_Price)) + 
  geom_line(stat = "density", 
            trim = TRUE) + 
  geom_line(data = ames_test, 
            stat = "density", 
            trim = TRUE, col = "red")
```

]

.pull-right[

```{r ames-split-dists-dist, echo = FALSE, fig.width=6, fig.height=4.25,  out.width = '100%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}
ggplot(ames_train, aes(x = Sale_Price)) + 
  geom_line(stat = "density", trim = TRUE) + 
  geom_line(data = ames_test, 
            stat = "density", 
            trim = TRUE, col = "red") 
```

]


---
layout: false
class: inverse, middle, center

#  Creating Models in R


---

# Specifying Models in R Using Formulas

To fit a model to the housing data, the model terms must be specified. Historically, there are two main interfaces for doing this. 

The **formula** interface using R [formula rules](https://cran.r-project.org/doc/manuals/r-release/R-intro.html#Formulae-for-statistical-models) to specify a _symbolic_ representation of the terms:

Variables + interactions

```{r formula-1, eval = FALSE}
model_fn(Sale_Price ~ Neighborhood + Year_Sold + Neighborhood:Year_Sold, data = ames_train)
```

Shorthand for all predictors

```{r formula-2, eval = FALSE}
model_fn(Sale_Price ~ ., data = ames_train)
```

Inline functions / transformations

```{r formula-3, eval = FALSE}
model_fn(log10(Sale_Price) ~ ns(Longitude, df = 3) + ns(Latitude, df = 3), data = ames_train)
```

This is very convenient but it has some disadvantages.  

---

# Downsides to Formulas

* You can't nest in-line functions such as `model_fn(y ~ pca(scale(x1), scale(x2), scale(x3)), data = dat)`.

* All the model matrix calculations happen at once and can't be recycled when used in a model function. 

* For very _wide_ data sets, the formula method can be [extremely inefficient](https://rviews.rstudio.com/2017/03/01/the-r-formula-method-the-bad-parts/). 

* There are limited _roles_ that variables can take which has led to several re-implementations of formulas. 

* Specifying multivariate outcomes is clunky and inelegant.

* Not all modeling functions have a formula method (consistency!). 

---

# Specifying Models Without Formulas

Some modeling function have a non-formula (XY) interface. This usually has arguments for the predictors and the outcome(s):

```{r non-formula, eval = FALSE}
# Usually, the variables must all be numeric
pre_vars <- c("Year_Sold", "Longitude", "Latitude")
model_fn(
  x = ames_train[, pre_vars],
  y = ames_train$Sale_Price
)
```

This is inconvenient if you have transformations, factor variables, interactions, or any other operations to apply to the data prior to modeling. 

Overall, it is difficult to predict if a package has one or both of these interfaces. For example, `lm` only has formulas. 

There is a **third interface**, using _recipes_ that will be discussed later that solves some of these issues. 

---

# A Linear Regression Model <img src="images/broom.png" class="title-hex">

Let's start by fitting an ordinary linear regression model to the training set. You can choose the model terms for your model, but I will use a very simple model:

```{r lm-1}
simple_lm <- lm(log10(Sale_Price) ~ Longitude + Latitude, data = ames_train)
```

Before looking at coefficients, we should do some model checking to see if there is anything obviously wrong with the model. 

To get the statistics on the individual data points, we will use the awesome `broom` package:

```{r lm-broom, warning= FALSE, message= FALSE}
simple_lm_values <- augment(simple_lm)
names(simple_lm_values)
``` 

---

# parsnip <img src="images/parsnip.png" class="title-hex">

- A tidy unified _interface_ to models

- `lm()` isn't the only way to perform linear regression
  
  - `glmnet` for regularized regression
  
  - `stan` for Bayesian regresion
  
  - `keras` for regression using tensorflow
  
- But...each interface has its own minutae to remember `r emo::ji("cry")`
  
  - `parsnip` standardizes all that!
  
---

# parsnip in Action <img src="images/parsnip.png" class="title-hex">

.pull-left[

1) Create specification

2) Set the engine

3) Fit the model

```{r}
spec_lin_reg <- linear_reg()
spec_lin_reg

spec_lm <- set_engine(spec_lin_reg, "lm")
spec_lm
```

]

.pull-right[

```{r}
fit_lm <- fit(
  spec_lm,
  log10(Sale_Price) ~ Longitude + Latitude,
  data = ames_train
)

fit_lm
```

]

---

# Different Interfaces <img src="images/parsnip.png" class="title-hex">

`parsnip` is not picky about the interface used to specify terms. Remember, `lm()` only allowed the formula interface!

```{r}
ames_train_log <- ames_train %>%
  mutate(Sale_Price_Log = log10(Sale_Price))

fit_xy(
  spec_lm,
  y = ames_train_log$Sale_Price_Log,
  x = ames_train_log[, c("Latitude", "Longitude")]
)
```


---

# Alternative Engines <img src="images/parsnip.png" class="title-hex">

With `parsnip`, it is easy to switch to a different engine, like Stan, to run the
same model with alternative backends.

.pull-left[

```{r, results = "hide", cache=TRUE}
spec_stan <- 
  spec_lin_reg %>%
  # Engine specific arguments are passed through here
  set_engine("stan", chains = 4, iter = 1000)

# Otherwise, looks exactly the same!
fit_stan <- fit(
  spec_stan,
  log10(Sale_Price) ~ Longitude + Latitude,
  data = ames_train
)
```

]

.pull-right[

```{r}
coef(fit_stan$fit)

coef(fit_lm$fit)
```

]

---
layout: false
class: inverse, middle, center

#  Model Evaluation


---

# Overall Model Statistics 

`parsnip` holds the actual model object in the `fit_lm$fit` slot. If you use the `summary()` method on the underlying `lm` object, the bottom shows some statistics: 

```{r lm-stats, eval = FALSE}
summary(fit_lm$fit)
```

```{r lm-stats-disp, echo = FALSE}
summary_res <- capture.output(summary(fit_lm$fit))
nlines <- length(summary_res)
summary_res <- summary_res[grep("Residual standard error", summary_res):(nlines-1)]
summary_res <- c("<snip>", summary_res)
cat(summary_res, sep = "\n")
```

These statistics are generated from _predicting on the training data used to fit the model_. This is problematic because it can lead to optimistic results, especially for flexible models (overfitting). 

--

### Idea!

The tests set is used for assessing performance. **Should we predict the test set** and use those results to estimate these statistics? 

---

<img src="images/nope.png" align = "middle" height = "400px" float = "center">

(Matthew Inman/Exploding Kittens)

---

# Assessing Models

_Save the test set_ until the very end when you have one or two models that are your favorite. We need to use the training set...but how?

--

.pull-left[

### Maybe... 

1) For model A, fit on training set, predict on training set

2) For model B, fit on training set, predict on training set

3) Compare performance

]

--

.pull-right[

For some models, it is possible to get very "good" performance by predicting the training set (it was so flexible you overfit it). That's an issue since we will need to make "honest" comparisons between models before we finalize them and run our final choices on the test set.

### If only...

If only we had a method for getting honest performance estimates from the _training set_...

]

---

# Resampling Methods

.pull-left[
These are additional data splitting schemes that are applied to the _training_ set. 

They attempt to simulate slightly different versions of the training set. These versions of the original are split into two model subsets:

* The _analysis set_ is used to fit the model (analogous to the training set). 
* Performance is determined using the _assessment set_. 

This process is repeated many times. 

There are different flavors or resampling but we will focus on two methods. 

]

.pull-right[

```{r split-graph, out.width = '100%', echo = FALSE, message=FALSE, warning=FALSE}
grViz("
digraph resampling_diag {
      
graph [layout = dot, bgcolor = transparent]

node [fontname = Helvetica]

all [shape = circle,
     label = 'All\nData']

te [shape = circle,
    style = filled,
    label = 'Testing',
    fillcolor = '#eeeeb4']

tr [shape = circle,
    style = filled,
    label = 'Training',
    fillcolor = '#c8d8c2']

r1 [shape = rectangle,
    label = 'Resample 1']

an1 [shape = oval,
     style = filled,
     label = 'Analysis',
     fillcolor = honeydew]
as1 [shape = oval,
     style = filled,
     label = 'Assessment',
     fillcolor = ivory]

r2 [shape = rectangle,
    label = 'Resample 2']
      
an2 [shape = oval,
     style = filled,
     label = 'Analysis',
     fillcolor = honeydew]

as2 [shape = oval,
     style = filled,
     label = 'Assessment',
     fillcolor = ivory]

r3 [shape = rectangle,
    label = 'Resample 3']
      
an3 [shape = oval,
     style = filled,
     label = 'Analysis',
     fillcolor = honeydew]
as3 [shape = oval,
     style = filled,
     label = 'Assessment',
     fillcolor = ivory]

all -> {tr te }
tr -> {r1 r2 r3 }
r1 -> {as1 an1 }
r2 -> {an2 as2 }
r3 -> {an3 as3 }
}
")
```

]

---

# V-Fold Cross-Validation

.pull-left[

Here, we randomly split the training data into _V_ distinct blocks of roughly equal size.

* We leave out the first block of analysis data and fit a model.

* This model is used to predict the held-out block of assessment data.

* We continue this process until we've predicted all _V_ assessment blocks

The final performance is based on the hold-out predictions by _averaging_ the statistics from the _V_ blocks. 

]

.pull-right[

```{r rs-diagram, echo = FALSE, out.width = '95%', fig.width=6, fig.height=2.5, fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}
grd <- 
  expand.grid(Resample = 1:5, Set = 1:5) %>%
  mutate(
    Purpose = ifelse(Resample == Set, "Performance", "Modeling"),
    Resample = factor(paste("Resample", Resample), levels = paste("Resample", 5:1))
  ) 

ggplot(grd, aes(x = Set, y = Resample, fill = Purpose)) + 
  geom_tile(width = 0.90, height = 0.85) + 
  xlab("< ------------------------ Random Data Groupings ------------------------>") + 
  ylab("") + 
  scale_fill_manual(values=c("#F46D43", "#dedede"))+ 
  thm + 
  theme(
    axis.text.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.ticks.y = element_blank(),
    panel.grid.major = element_blank(),
    panel.grid.minor = element_blank(),
    panel.border = element_blank()
  )
```

<br>

_V_ is usually taken to be 5 or 10 and leave one out cross-validation has each sample as a block. 


]

---

#  10-Fold Cross-Validation with _n_ = 50

```{r cv-plot, echo = FALSE, message = FALSE, fig.width=6, fig.height=4.25,  out.width = '60%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}

set.seed(4121)
cv10 <- vfold_cv(ames[1:50, ])
cv10 <- tidy(cv10) %>%
  mutate(Row = factor(Row, levels = paste(1:50))) 

ggplot(cv10, aes(x = Fold, y = Row, fill = Data)) + 
  geom_tile() + scale_fill_brewer(palette = "Paired") + 
  xlab("") + ylab("Training Set Sample") + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        axis.text.y = element_text(size = rel(.5)), 
        legend.position = "top",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
```


---

# Bootstrapping

A bootstrap sample is the _same size_ as the training set but each data point is selected _with replacement_. 

* _Analysis set_

  Will contain more than one replicate of a training set instance.
  
* _Assessment set_

  Contains all samples that were never included in the corresponding bootstrap set.
  Often called the "out-of-bag" sample and can vary in size!

On average, `r (1-exp(-1))*100`% of the training set is contained _at least once_ in the bootstrap sample. 

---

#  Bootstrapping with _n_ = 50

```{r boot-plot, echo = FALSE, warning=FALSE, message = FALSE, fig.width=6, fig.height=4.25,  out.width = '60%', fig.align='center', dev = 'svg', dev.args = list(bg = "transparent")}
set.seed(4121)
bt_rows <- bootstraps(ames[1:50, ], times = 10)

bt_rows <- data.frame(Row = unlist(lapply(bt_rows$splits, function(x) sort(x$in_id))),
                      Resample = rep(recipes:::names0(10, "Resample"), each = 50))

bt_rows <- bt_rows %>%
  group_by(Resample, Row) %>%
  summarize(Replicates = length(Row)) %>%
  mutate(Row = factor(Row, levels = paste(1:50)),
         Replicates = factor(Replicates))

ggplot(bt_rows, aes(x = Resample, y = Row, fill = Replicates)) + 
  geom_tile() + scale_fill_brewer() + 
  theme(panel.grid.major = element_blank(),
        panel.grid.minor = element_blank()) + 
  xlab("") + ylab("Training Set Sample") + 
  theme_bw() + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1), 
        axis.text.y = element_text(size = rel(.5)), 
        legend.position = "top",
        panel.grid.major = element_blank(),
        panel.grid.minor = element_blank())
```

---

# Cross-Validating Using `rsample` <img src="images/rsample.png" class="title-hex">

.pull-left[

```{r cv-ames}
set.seed(2453)
cv_splits <- vfold_cv(
  data = ames_train, 
  v = 10, 
  strata = "Sale_Price"
)
cv_splits %>% slice(1:6)
```

]

.pull-right[

Each individual split object is similar to the 
`initial_split()` example.

```{r cv-ames-splits}
cv_splits$splits[[1]]

cv_splits$splits[[1]] %>% analysis() %>% dim()
cv_splits$splits[[1]] %>% assessment() %>% dim()
```

]

???

Note that `<split [2K/222]>` rounds to the thousandth and is the same as `<1977/222/2199>`


---

# Resampling the Linear Model <img src="images/rsample.png" class="title-hex"><img src="images/purrr.png" class="title-hex"><img src="images/parsnip.png" class="title-hex"><img src="images/dplyr.png" class="title-hex">

Working with resample tibbles generally involves two things:

1) Small functions that perform an action on a single split.

2) The `purrr` package for `map()`ping over splits.

.pull-left[

```{r, eval=FALSE}
geo_form <- log10(Sale_Price) ~ Latitude + Longitude

# Fit on a single analysis resample
fit_model <- function(split, spec) {
  fit(
    object = spec, 
    formula = geo_form,
    data = analysis(split) # <- pull out training set
  )
}

# For each resample, call fit_model()
cv_splits <- cv_splits %>% 
  mutate(models_lm = map(splits, fit_model, spec_lm))

cv_splits
```

]

.pull-right[

```{r, echo=FALSE}
geo_form <- log10(Sale_Price) ~ Latitude + Longitude

# Fit on a single analysis resample
fit_model <- function(split, spec) {
  fit(
    object = spec, 
    formula = geo_form,
    data = analysis(split) # <- pull out training set
  )
}

# For each resample, call fit_model()
cv_splits <- cv_splits %>% 
  mutate(models_lm = map(splits, fit_model, spec_lm))

cv_splits
```

]

???

Note that `<fit[+]>` means not model fitting failures.


---

# Resampling the Linear Model <img src="images/purrr.png" class="title-hex"><img src="images/parsnip.png" class="title-hex"><img src="images/dplyr.png" class="title-hex">

Next, we will attach the predictions for each resample:

.pull-left[

```{r, compute-pred}
compute_pred <- function(split, model) {
  
  # Extract the assessment set
  assess <- assessment(split) %>%
    mutate(Sale_Price_Log = log10(Sale_Price))
  
  # Compute predictions (a df is returned)
  pred <- predict(model, new_data = assess)
  
  bind_cols(assess, pred)
}
```

]

.pull-right[

```{r purrr-lm-pred}
cv_splits <- cv_splits %>%
  mutate(
    pred_lm = map2(splits, models_lm, compute_pred)
  )
cv_splits
```

]


---

# Resampling the Linear Model <img src="images/purrr.png" class="title-hex"><img src="images/parsnip.png" class="title-hex"><img src="images/dplyr.png" class="title-hex">

Now, let's compute two performance measures:

.pull-left[

```{r purrr-lm-results}
compute_perf <- function(pred_df) {
  
  # Create a function that calculates
  # rmse and rsq and returns a data frame
  numeric_metrics <- metric_set(rmse, rsq)
  
  numeric_metrics(
    pred_df, 
    truth = Sale_Price_Log, 
    estimate = .pred
  )
  
}
```

]

.pull-right[
```{r purrr-lm-estimate}
cv_splits <- cv_splits %>%
  mutate(perf_lm = map(pred_lm, compute_perf))

select(cv_splits, pred_lm, perf_lm)
```
]

---

# Resampling the Linear Model <img src="images/purrr.png" class="title-hex"><img src="images/parsnip.png" class="title-hex"><img src="images/dplyr.png" class="title-hex">

And finally, let's compute the average of each metric over the resamples:

.pull-left[

```{r}
cv_splits$perf_lm[[1]]
```

]

.pull-right[

```{r}
cv_splits %>%
  unnest(perf_lm) %>%
  group_by(.metric) %>%
  summarise(
    .avg = mean(.estimate),
    .sd = sd(.estimate)
  )
```

]

---

# What Was the Ruckus?

```{r, include=FALSE}
rmse_cv <- cv_splits$perf_lm %>%
  bind_rows() %>%
  filter(.metric == "rmse") %>%
  pull(.estimate) %>%
  mean()
```

Previously, I mentioned that the performance metrics that were naively calculated from the training set could be optimistic. However, this approach estimates the RMSE to be `r round(summary(simple_lm)$sigma, 4)` and cross-validation produced an estimate of `r round(rmse_cv, 4)`. What was the big deal? 

Linear regression is a _high bias model_. This means that it is fairly incapable at being able to adapt the underlying model function (unless it is linear). For this reason, linear regression is unlikely to **overfit** to the training set and our two estimates are likely to be the same. 

---

# What Was the Ruckus?

There are other models, such as _K-Nearest Neighbors_ that are  _low bias_ since they can, theoretically, easily adapt to a wide variety of true model functions. 

A similar analysis using a K-Nearest Neighbors model with 2 neighbors shows a **95% $R^2$** in-sample, but a **68% $R^2$** when resampled.

There is also variance to consider. Linear regression is very stable since it leverages all of the data points to estimate parameters. Other methods, such as _tree-based models_, are not and can drastically change if the training set data is slightly perturbed. 

**tl;dr**: The earlier concern is real, but linear regression is less likely to be affected.  

**tl;dr2**: Resampling let's you shoot yourself in the foot with a nerfgun, rather than a shotgun.

---

# What did we learn?

- `parsnip` for standardized model fitting

- `rsample` for data splitting

  - Training / Test set
  
  - Resampling (cross validation, bootstraps)

- `purrr` and list-columns for multiple models

## What else?

* [`http://www.tidyverse.org/`](http://www.tidyverse.org/)
* [R for Data Science](http://r4ds.had.co.nz/)

About these slides.... they were created with Yihui's [`xaringan`](https://github.com/yihui/xaringan) and the stylings are a slightly modified version of Patrick Schratz's [Metropolis theme](https://github.com/pat-s/xaringan-metropolis).

---
layout: false
class: inverse, middle, center

# Thank You!